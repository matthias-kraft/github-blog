---
title: "Is AI making the same mistakes as us?"
date: 2021-04-23
author: "Matthias Kraft"
linktitle: Is AI making the same mistakes as us?
weight: 1
avatarPicture: image/cartoon_mk.png
image: /image/niv-singer-banksy.jpg
tags : [
"safeai",
"LakeraAI"
]
categories : [
"blog"
]
draft: false
---

# Is AI making the same mistakes as us?


I watched the Netflix documentary [CodedBias](https://www.codedbias.com/) last weekend. And it raised some rather good points about AI development. For those of you who haven’t yet seen the film, it tells the story of a group of inspiring women fighting against the injustices of AI. The documentary feels like a natural follow-up to the film [SocialDilemma](https://www.thesocialdilemma.com/). And, if you were horrified by the practices uncovered in it, I strongly recommend you also watch [CodedBias](https://www.codedbias.com/)!

Rather than exploring Terminator-style takeover scenarios by a super-intelligent AI, [CodedBias](https://www.codedbias.com/) takes a hard look at the real problems created by AI technologies today. It does a great job at highlighting the issue of bias in AI, and how that adversely affects whole demographics. The point is most strikingly illustrated by the example of a recidivism algorithm [discriminating against Black defendants](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). We don’t need to look very far either to find instances of bias in AI beyond those brought up in the documentary. Even Google Translate, which is a fairly innocent looking service for the public benefit, can exhibit [biases](https://twitter.com/DoraVargha/status/1373211762108076034?s=20) (see image below). Ultimately, bias in AI is a real issue. And as the consumers and subjects of these algorithms, this should worry us all.

![Translate](/image/GoogleHungarianEnglishGenderBiasedTranslation.png)*Example of AI bias in translating from a gender neutral language, Hungarian, to English.[^Screenshot]*

So, rightly, the documentary takes a look at the bigger picture of how these technologies are being used. Even if it was free of inherent bias, is AI-powered video surveillance really the way to go? Do we want to enable corporate powers to predict our behaviour with ever-increasing accuracy by giving them our data? These are questions that we, as a society, have to answer.

# Is developing AI worth the risk?

Due to the risks outlined in [CodedBias](https://www.codedbias.com/), one might be left wondering – is using AI worth it? The answer is an emphatic YES. This might come as a surprise to those of you who have seen [CodedBias](https://www.codedbias.com/), since the documentary sacrifices this side of the discussion to achieve cohesive storytelling. In reality, the potential to do social good using AI is enormous. Even before we perfect self-driving cars, AI-based advanced driver-assistance systems will save human lives. AI advances in radiology will assist doctors in cancer diagnosis and allow the development of more personalised treatment plans. AI will speed up and improve drug discovery, giving us the chance to treat and cure more diseases. In agriculture, AI lets farmers monitor the health of their crops to an unprecedented level of accuracy, which enables more sustainable farming. And the list goes on.

# The path forward - how to ensure unbiased AI?
The question is, how can we reap the benefits of AI while simultaneously eliminating the downsides? To solve this problem, [CodedBias](https://www.codedbias.com/) makes a case for the necessity of regulations. What we need is a sort of “FDA[^FDA] for algorithms”[^Cathy]. In a pledge, the makers of [CodedBias](https://www.codedbias.com/) demand that:  

<cite>“... algorithmic systems be vetted for accuracy, bias, and non-discrimination, evaluated for harms and capacity for abuse, and subject to continuous scrutiny.”[^pledge]</cite>


This is a call that I fully support! When AI algorithms are used to assess our teachers or to determine our credit worthiness, when they impact human lives or society at any scale, we should expect them to be regulated. I am glad that efforts to create frameworks for compliance are already being developed by regulators in [healthcare](https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device), [aviation](https://www.easa.europa.eu/sites/default/files/dfu/easa_concept_paper_first_usable_guidance_for_level_1_machine_learning_applications_-_proposed_issue_01_1.pdf), automotive, and other industries. Though I must say, I don’t envy those regulators because they are facing an incredibly tough job. If they make the proposed regulations too lax, society will lose as a result. If regulations are made too strict, they won’t allow innovation and, again, society will lose. And if they end up being so complex that only big players can follow them – you guessed it – society will lose. To walk this fine line of not too lax, not too strict and not too complex regulations, lawmakers would benefit in developing these through short, use-case driven projects involving companies of various sizes and researchers alike.   

Of course, regulations are not a silver bullet that will immediately solve all AI-related issues. And the problem is much too complex to be solved by a single stakeholder. Regulators can set the process and expectations, but they cannot solve the underlying technical difficulties. Researchers have much work to do in investigating all the ways bias can creep into AI algorithms. Engineers carry the responsibility to build the necessary tools to not only detect bias, but to ensure [AI systems work as intended](https://david-haber.github.io/posts/ai-discipline/). Indeed, it will require the concerted effort by many, including society as a whole, to create truly trustworthy AI. I’m optimistic that we will rise to the challenge. And in the meantime, I recommend that you watch [CodedBias](https://www.codedbias.com/), in order to get a better understanding about the wider issues surrounding AI!

*Thanks to [Mateo](https://www.linkedin.com/in/mateor/), [David](https://www.linkedin.com/in/haberdavid/) and Ina for reading and commenting on drafts of this article. Image credits to [Niv Singer](https://unsplash.com/@niv) and [unsplash](https://unsplash.com/). #safeai [#LakeraAI](https://lakera.ai/)*

[^Screenshot]:Translation and screenshot created on 23.4.2021, the results of the translation may have changed in the meantime.
[^FDA]: U.S. Food and Drug Administration.
[^Cathy]: Quote from Cathy O’Neil in the documentary [CodedBias](https://www.codedbias.com/).
[^pledge]: https://www.codedbias.com/sign
